{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very similar to the EMNIST Model Collapse code, using CNN clasifiers rather than fully connected ones.\n",
    "\n",
    "Originally intended to increase the amount of compute dedicated to VAE training. Experiments show that this approach may increase the performance of classifier accuracy evlaution and thus give a better metric of model collapse. However, these improvements are quite minimal, and likely not worth the additional computational time and code complexity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import torch.nn.functional as F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters. Including detail of CNN design\n",
    "\n",
    "# CNN hyperparameters\n",
    "cnn_channels = [32, 64, 128]  # Number of channels in each convolutional layer\n",
    "cnn_kernel_sizes = [3, 3, 3]  # Kernel sizes for each convolutional layer\n",
    "cnn_strides = [1, 1, 1]  # Strides for each convolutional layer\n",
    "cnn_paddings = [1, 1, 1]  # Paddings for each convolutional layer\n",
    "use_pooling = True  # Whether to use max pooling after each conv layer\n",
    "pool_size = 2  # Size of the max pooling window\n",
    "fc_layers = [256, 128]  # Fully connected layers after convolutions\n",
    "\n",
    "# Training hyperparameters\n",
    "batch_size = 128\n",
    "latent_dim = 50 # VAAE latent dimension\n",
    "vae_hidden_layers = [256, 128, 64] # note the VAE has a symmetric encode / decoder structure. However, weights are not tied\n",
    "classifier_hidden_dim = 512 # hidden dimension in the classifier\n",
    "learning_rate = 5e-4 \n",
    "vae_num_epochs = 200 \n",
    "classifier_num_epochs = 100 # Larger numbers required for EMNIST\n",
    "num_iterations = 25 \n",
    "num_samples = 112800  \n",
    "num_images = 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load balanced EMNIST dataset\n",
    "\n",
    "# Load original EMNIST dataset\n",
    "dataset_mean = 0.1751\n",
    "dataset_std = 0.3332\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((dataset_mean,), (dataset_std,)) # mean and variance. See later cells to see how these were calculated\n",
    "    # normalisation accelerates convergence of the VAE, and possibly of the classifier. However, it means MSE as opposed to BCE must be used\n",
    "])\n",
    "\n",
    "original_dataset = torchvision.datasets.EMNIST(root='./data', split='balanced', train=True, download=True, transform=transform) # separate download each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE architecture specified\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_layers:\n",
    "            encoder_layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU()\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        \n",
    "        self.fc_mu = nn.Linear(hidden_layers[-1], latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_layers[-1], latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        prev_dim = latent_dim\n",
    "        for hidden_dim in reversed(hidden_layers):\n",
    "            decoder_layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU()\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        decoder_layers.append(nn.Linear(hidden_layers[0], input_dim))\n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN classifier design\n",
    "\n",
    "# Note that the classifier is trained using cross entropy\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_channels, input_height, input_width, num_classes, \n",
    "                 cnn_channels, cnn_kernel_sizes, cnn_strides, cnn_paddings, \n",
    "                 use_pooling, pool_size, fc_layers):\n",
    "        super(Classifier, self).__init__()\n",
    "        \n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        self.use_pooling = use_pooling\n",
    "        \n",
    "        # Convolutional layers\n",
    "        for i in range(len(cnn_channels)):\n",
    "            in_channels = input_channels if i == 0 else cnn_channels[i-1]\n",
    "            self.conv_layers.append(nn.Conv2d(in_channels, cnn_channels[i], \n",
    "                                              kernel_size=cnn_kernel_sizes[i], \n",
    "                                              stride=cnn_strides[i], \n",
    "                                              padding=cnn_paddings[i]))\n",
    "            if use_pooling:\n",
    "                self.conv_layers.append(nn.MaxPool2d(pool_size))\n",
    "        \n",
    "        # Calculate the size of the feature maps after convolutions and pooling\n",
    "        def conv_output_size(size, kernel_size, stride, padding):\n",
    "            return (size - kernel_size + 2 * padding) // stride + 1\n",
    "        \n",
    "        for i in range(len(cnn_channels)):\n",
    "            input_height = conv_output_size(input_height, cnn_kernel_sizes[i], cnn_strides[i], cnn_paddings[i])\n",
    "            input_width = conv_output_size(input_width, cnn_kernel_sizes[i], cnn_strides[i], cnn_paddings[i])\n",
    "            if use_pooling:\n",
    "                input_height //= pool_size\n",
    "                input_width //= pool_size\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        prev_dim = cnn_channels[-1] * input_height * input_width\n",
    "        for fc_size in fc_layers:\n",
    "            self.fc_layers.append(nn.Linear(prev_dim, fc_size))\n",
    "            prev_dim = fc_size\n",
    "        self.fc_layers.append(nn.Linear(prev_dim, num_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Convolutional layers\n",
    "        for layer in self.conv_layers:\n",
    "            x = F.relu(layer(x))\n",
    "        \n",
    "        # Flatten the output\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        for layer in self.fc_layers[:-1]:\n",
    "            x = F.relu(layer(x))\n",
    "        \n",
    "        # Final layer (no activation)\n",
    "        x = self.fc_layers[-1](x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train VAE function\n",
    "\n",
    "# VAE loss function\n",
    "def vae_loss_function(recon_x, x, mu, logvar): \n",
    "    MSE = nn.functional.mse_loss(recon_x, x.view(-1, 784), reduction='sum') # MSE required. BCE can be used if normalisation is not introduced\n",
    "    # BCE makes more theoretical sense, but seems irrelevant in practice\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) # KLD between gaussians. Assuming roughly gaussian structure\n",
    "    return MSE + KLD\n",
    "\n",
    "# Train VAE\n",
    "def train_vae(vae, optimizer, dataloader):\n",
    "    vae.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = vae(data) # reconstructed batch straight from vae.forward\n",
    "        loss = vae_loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward() # fails when reparameterisation isn't implemented appropriately\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    return train_loss / len(dataloader.dataset) # necessary for stability if num_samples is not equal to 60,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train classifier\n",
    "\n",
    "def train_classifier(classifier, optimizer, dataloader):\n",
    "    classifier.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in dataloader:\n",
    "        data = data.view(-1, 1, 28, 28)  # Reshape to (batch_size, channels, height, width)\n",
    "        optimizer.zero_grad()\n",
    "        output = classifier(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    return train_loss / len(dataloader.dataset), correct / len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate, classify, and visualise digits\n",
    "\n",
    "# Function to generate new digits. Note that this is for training the next iteration of the models. The visualisation occurs elsewhere, and displays the first num_images of these generated samples\n",
    "def generate_digits(vae, num_samples):\n",
    "    vae.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_samples, latent_dim)\n",
    "        samples = vae.decode(z)\n",
    "    return samples\n",
    "\n",
    "# This could probably be done by specifying regions in latent space instead. How could I do this?\n",
    "def classify_digits(classifier, digits, batch_size=batch_size):\n",
    "    classifier.eval()\n",
    "    dataset = TensorDataset(digits)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = batch[0].view(-1, 1, 28, 28)  # Reshape to (batch_size, channels, height, width)\n",
    "            output = classifier(batch)\n",
    "            labels = output.argmax(dim=1)\n",
    "            all_labels.append(labels)\n",
    "    \n",
    "    return torch.cat(all_labels)\n",
    "\n",
    "\n",
    "def visualize_digits(digits, labels, iteration): # removed the original EMINST code and replaced it with MNIST work. Should work fine?\n",
    "    num_digits = len(digits)\n",
    "    num_rows = math.ceil(num_digits / 10) # required  to allow arbitrary numbers to be shown\n",
    "    # Currently creates a single image out of all of them with 10 columns, and the digits stacked on top of one another\n",
    "    num_cols = min(num_digits, 10)\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(2*num_cols, 2*num_rows))\n",
    "    fig.suptitle(f'Generated Digits - Iteration {iteration}')\n",
    "\n",
    "    balanced_dict = {0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8', 9: '9', 10: 'A', 11: 'B', 12: 'C', 13: 'D', 14: 'E', 15: 'F', 16: \n",
    "                 'G', 17: 'H', 18: 'I', 19: 'J', 20: 'K', 21: 'L', 22: 'M', 23: 'N', 24: 'O', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'T', 30: 'U', 31: 'V', \n",
    "                 32: 'W', 33: 'X', 34: 'Y', 35: 'Z', 36: 'a', 37: 'b', 38: 'd', 39: 'e', 40: 'f', 41: 'g', 42: 'h', 43: 'n', 44: 'q', 45: 'r', 46: 't'}\n",
    "\n",
    "    if num_rows == 1:\n",
    "        axes = axes.reshape(1, -1) # avoid index errors\n",
    "\n",
    "    for i in range(num_digits):\n",
    "        row = i // 10\n",
    "        col = i % 10\n",
    "        ax = axes[row, col]\n",
    "        image = digits[i].reshape(28, 28)\n",
    "        rotated_image = torch.rot90(image, k=-1)\n",
    "        flipped_image = torch.flip(rotated_image, dims=[1])\n",
    "        ax.imshow(flipped_image.squeeze(), cmap='gray') # recreates original MNIST images as faithfully as possible. Perhaps a custom cmap would be better for historic accuracy?\n",
    "        ax.set_title(f'Label: {balanced_dict[labels[i].item()]}') # NOTE: Printed images are not sorted by label in any way. This could be resolved in future?\n",
    "        ax.axis('off')\n",
    "\n",
    "    # Remove any unused subplots. Without this, there may be graphical glitches and poorly placed images\n",
    "    for i in range(num_digits, num_rows * num_cols):\n",
    "        row = i // 10\n",
    "        col = i % 10\n",
    "        fig.delaxes(axes[row, col])\n",
    "\n",
    "    plt.tight_layout() # I might want this. It seems to place the title too low, such that it overlaps with the images themselves. Removing this fixes the title issue\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate a given classifier and dataloader\n",
    "\n",
    "# Update the evaluate_classifier function\n",
    "def evaluate_classifier(classifier, dataloader):\n",
    "    classifier.eval()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in dataloader:\n",
    "            data = data.view(-1, 1, 28, 28)  # Reshape to (batch_size, channels, height, width)\n",
    "            output = classifier(data)\n",
    "            loss = criterion(output, target)\n",
    "            total_loss += loss.item() * data.size(0)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            total += data.size(0)\n",
    "    avg_loss = total_loss / total\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise loss and accuracy matrices of various classifiers\n",
    "\n",
    "# Visualize the loss and accuracy matrices\n",
    "def plot_matrix(matrix, title, cmap='viridis'): # I have decided I have viridis\n",
    "    plt.figure(figsize=(15, 12)) # made larger than previous versions. To help with readability of text\n",
    "    plt.imshow(matrix, cmap=cmap)\n",
    "    plt.colorbar() # considerably better visualisation\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Dataset Iteration')\n",
    "    plt.ylabel('Classifier Iteration')\n",
    "    for i in range(matrix.shape[0]): # NOTE: Currently counts from 0. I should probably shift it to counting from 1, like all humans do\n",
    "        for j in range(matrix.shape[1]):\n",
    "            plt.text(j, i, f'{matrix[i, j]:.2f}', ha='center', va='center', color='white')\n",
    "    #plt.tight_layout() # produces graphical bugs with the title, but is clearer with the colourbar and labels\n",
    "    plt.show()\n",
    "\n",
    "# Visualise the loss and accuracy matrices without text. Just colour plots\n",
    "def colour_plot_matrix(matrix, title, cmap='viridis', num_ticks=11):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(matrix, cmap=cmap)\n",
    "    plt.colorbar()\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Dataset Iteration')\n",
    "    plt.ylabel('Classifier Iteration')\n",
    "    \n",
    "    # Create evenly spaced tick locations\n",
    "    x_ticks = np.linspace(0, matrix.shape[1] - 1, num_ticks, dtype=int)\n",
    "    y_ticks = np.linspace(0, matrix.shape[0] - 1, num_ticks, dtype=int)\n",
    "    \n",
    "    # Set tick locations and labels\n",
    "    plt.xticks(x_ticks, x_ticks + 1)  # +1 to start from 1 instead of 0\n",
    "    plt.yticks(y_ticks, y_ticks + 1)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/25\n"
     ]
    }
   ],
   "source": [
    "# Main loop. Iterates over training of models and generating samples for new models\n",
    "\n",
    "current_dataset = original_dataset\n",
    "classifiers = []\n",
    "loss_matrix = np.zeros((num_iterations, num_iterations)) \n",
    "accuracy_matrix = np.zeros((num_iterations, num_iterations))\n",
    "\n",
    "vae = VAE(784, vae_hidden_layers, latent_dim) # VAE is not reset after each iteration to save on computational time\n",
    "for iteration in range(num_iterations):\n",
    "    print(f\"Iteration {iteration + 1}/{num_iterations}\")\n",
    "\n",
    "    dataloader = DataLoader(current_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    vae_optimizer = optim.Adam(vae.parameters(), lr=learning_rate)\n",
    "    for epoch in range(vae_num_epochs):\n",
    "        loss = train_vae(vae, vae_optimizer, dataloader)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'VAE Epoch {epoch+1}/{vae_num_epochs}, Loss: {loss:.4f}')\n",
    "\n",
    "    # Update the classifier initialization in the main loop\n",
    "    classifier = Classifier(input_channels=1, input_height=28, input_width=28, num_classes=47,\n",
    "                        cnn_channels=cnn_channels, cnn_kernel_sizes=cnn_kernel_sizes,\n",
    "                        cnn_strides=cnn_strides, cnn_paddings=cnn_paddings,\n",
    "                        use_pooling=use_pooling, pool_size=pool_size, fc_layers=fc_layers)\n",
    "    \n",
    "    classifier_optimizer = optim.Adam(classifier.parameters(), lr=learning_rate) # does this need to be respecified?\n",
    "    for epoch in range(classifier_num_epochs):\n",
    "        loss, accuracy = train_classifier(classifier, classifier_optimizer, dataloader)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Classifier Epoch {epoch+1}/{classifier_num_epochs}, Loss: {loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    # Store the trained classifier in classifiers list. Used later for overall evaluation\n",
    "    classifiers.append(classifier)\n",
    "\n",
    "    # Generate new dataset\n",
    "    generated_digits = generate_digits(vae, num_samples)\n",
    "    generated_labels = classify_digits(classifier, generated_digits) # from prev classifier notably\n",
    "\n",
    "    # Visualize some generated digits\n",
    "    visualize_digits(generated_digits[:num_images], generated_labels[:num_images], iteration + 1) \n",
    "\n",
    "\n",
    "    # Create new dataset for next iteration\n",
    "    current_dataset = TensorDataset(generated_digits, generated_labels) # torch.utils.data.TensorDataset\n",
    "\n",
    "    # Evaluate all previous classifiers on the new dataset, and writing to the loss and accuracy arrays\n",
    "    new_dataloader = DataLoader(current_dataset, batch_size=batch_size, shuffle=False) # shuffle should be unnecessary as the sampling from the latent space is random\n",
    "    for prev_iteration, prev_classifier in enumerate(classifiers):\n",
    "        loss, accuracy = evaluate_classifier(prev_classifier, new_dataloader)\n",
    "        loss_matrix[prev_iteration, iteration] = loss\n",
    "        accuracy_matrix[prev_iteration, iteration] = accuracy\n",
    "\n",
    "    # This line can be used to save the current dataset for later analysis. Requires drive loading\n",
    "    #torch.save(current_dataset, f'generated_dataset_iteration_{iteration + 1}.pt')\n",
    "\n",
    "# Save matrices for further analysis\n",
    "#np.save('loss_matrix_EMNIST.npy', loss_matrix)\n",
    "#np.save('accuracy_matrix_EMNIST.npy', accuracy_matrix)\n",
    "\n",
    "# Show relative loss and accuracies of classifiers on generated datasets\n",
    "# The plot_matrix function explicitly includes precise values. More appropriate for small numbers of iterations\n",
    "plot_matrix(loss_matrix, 'Loss Matrix')\n",
    "plot_matrix(accuracy_matrix, 'Accuracy Matrix')\n",
    "\n",
    "colour_plot_matrix(loss_matrix, 'Loss Matrix')\n",
    "colour_plot_matrix(accuracy_matrix, 'Accuracy Matrix')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
