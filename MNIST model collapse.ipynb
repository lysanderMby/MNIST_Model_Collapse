{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt # imshow might be faster?\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "\n",
    "batch_size = 128\n",
    "latent_dim = 20 # the smallest layer in the VAE. May be too high, as indicated by work analysing sparsity\n",
    "vae_hidden_dim = 128 # size of the hidden dimension in the vae\n",
    "classifier_hidden_dim = 256 # hidden dimension in the classifier\n",
    "learning_rate = 1e-3\n",
    "vae_num_epochs = 200 # higher slows collapse\n",
    "classifier_num_epochs = 100 # 100 is adequate to achieve 100% accuracy each time\n",
    "num_iterations = 50  # Number of times to repeat the process\n",
    "num_samples = 60000  # Number of samples to generate in each iteration # Note that original MNIST (all 60000) are still used regardless of this value for the first iteration\n",
    "num_images = 10 # This is for printing samples. Not actually relevant to the code logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original MNIST dataset\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)) # mean and variance. See later cells to see how these were calculated\n",
    "    # normalisation accelerates convergence of the VAE, and possibly of the classifier. However, it means MSE as opposed to BCE must be used\n",
    "])\n",
    "\n",
    "original_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform) # separate download each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the VAE model. Standard ReLU with one hidden layer\n",
    "\n",
    "# logvar emerges more naturally. Also useful for the explicit KLD\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar) # from N(0,1) assumption\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar # returns LOGVAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Classifier\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "# VAE loss function\n",
    "def vae_loss_function(recon_x, x, mu, logvar): # recon_x is short for reconstructed\n",
    "    MSE = nn.functional.mse_loss(recon_x, x.view(-1, 784), reduction='sum') # MSE required. BCE can be used if normalisation is not introduced\n",
    "    # BCE makes more theoretical sense, but seems irrelevant in practice\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) # closed form for KLD between gaussians\n",
    "    return MSE + KLD\n",
    "\n",
    "# Generates new digits for later training\n",
    "def generate_digits(vae, num_samples):\n",
    "    vae.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_samples, latent_dim)\n",
    "        samples = vae.decode(z)\n",
    "    return samples\n",
    "\n",
    "# Classify generated digits. Uses the previous iteration's classifier, bootstrapping future classifiers.\n",
    "def classify_digits(classifier, digits):\n",
    "    classifier.eval()\n",
    "    with torch.no_grad():\n",
    "        output = classifier(digits)\n",
    "        labels = output.argmax(dim=1)\n",
    "    return labels\n",
    "\n",
    "\n",
    "# Evaluates a classifier on a dataset\n",
    "# Used to test performance of historic classifiers on newly generated sets of images to test statistical distance\n",
    "def evaluate_classifier(classifier, dataloader):\n",
    "    classifier.eval()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad(): \n",
    "        for data, target in dataloader:\n",
    "            output = classifier(data.view(data.size(0), -1))\n",
    "            loss = criterion(output, target)\n",
    "            total_loss += loss.item() * data.size(0)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            total += data.size(0)\n",
    "    avg_loss = total_loss / total\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training functions\n",
    "\n",
    "# Train VAE\n",
    "def train_vae(vae, optimizer, dataloader):\n",
    "    vae.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = vae(data) # reconstructed batch straight from vae.forward\n",
    "        loss = vae_loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward() # fails when reparameterisation isn't implemented appropriately\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    return train_loss / len(dataloader.dataset) # necessary for stability if num_samples is not equal to 60,000\n",
    "\n",
    "# Train Classifier\n",
    "def train_classifier(classifier, optimizer, dataloader): # could pass the raw dataset to avoid dependencies?\n",
    "    classifier.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    train_loss = 0\n",
    "    correct = 0 # for later accuracy calculation. Entirely for evaluation\n",
    "    for data, target in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        output = classifier(data.view(data.size(0), -1)) # turn to an array\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        pred = output.argmax(dim=1, keepdim=True) # unclear how slow argmax is as a step, but it is certainly easier than implementing a for loop here\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    return train_loss / len(dataloader.dataset), correct / len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting functions\n",
    "\n",
    "def visualize_digits(digits, labels, iteration):\n",
    "    num_digits = len(digits)\n",
    "    num_rows = math.ceil(num_digits / 10) # allows arbitrarily many images to be shown. Displays these images in rows of 10\n",
    "    num_cols = min(num_digits, 10)\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(2*num_cols, 2*num_rows))\n",
    "    fig.suptitle(f'Generated Digits - Iteration {iteration}')\n",
    "\n",
    "    if num_rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "\n",
    "    for i in range(num_digits):\n",
    "        row = i // 10\n",
    "        col = i % 10\n",
    "        ax = axes[row, col]\n",
    "        ax.imshow(digits[i].view(28, 28).numpy(), cmap='gray') # recreates original MNIST images as faithfully as possible\n",
    "        ax.set_title(f'Label: {labels[i].item()}')\n",
    "        ax.axis('off')\n",
    "\n",
    "    # Remove any unused subplots\n",
    "    for i in range(num_digits, num_rows * num_cols):\n",
    "        row = i // 10\n",
    "        col = i % 10\n",
    "        fig.delaxes(axes[row, col])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the loss and accuracy matrices\n",
    "def plot_matrix(matrix, title, cmap='viridis'): # I have decided I have viridis\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(matrix, cmap=cmap)\n",
    "    plt.colorbar() # considerably better visualisation\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Dataset Iteration')\n",
    "    plt.ylabel('Classifier Iteration')\n",
    "    for i in range(matrix.shape[0]): # NOTE: Currently counts from 0. I should probably shift it to counting from 1, like all humans do\n",
    "        for j in range(matrix.shape[1]):\n",
    "            plt.text(j, i, f'{matrix[i, j]:.2f}', ha='center', va='center', color='white')\n",
    "    #plt.tight_layout() # produces graphical bugs with the title, but is clearer with the colourbar and labels\n",
    "    plt.show()\n",
    "\n",
    "# Creates a colour plot without including values. Far more appropriate for large runs\n",
    "def colour_plot_matrix(matrix, title, cmap='viridis', num_ticks=11):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(matrix, cmap=cmap)\n",
    "    plt.colorbar()\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Dataset Iteration')\n",
    "    plt.ylabel('Classifier Iteration')\n",
    "    \n",
    "    # Create evenly spaced tick locations\n",
    "    x_ticks = np.linspace(0, matrix.shape[1] - 1, num_ticks, dtype=int)\n",
    "    y_ticks = np.linspace(0, matrix.shape[0] - 1, num_ticks, dtype=int)\n",
    "    \n",
    "    # Set tick locations and labels\n",
    "    plt.xticks(x_ticks, x_ticks + 1)  # +1 to start from 1 instead of 0\n",
    "    plt.yticks(y_ticks, y_ticks + 1)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop\n",
    "\n",
    "# Initialisation\n",
    "current_dataset = original_dataset\n",
    "classifiers = []\n",
    "loss_matrix = np.zeros((num_iterations, num_iterations))\n",
    "accuracy_matrix = np.zeros((num_iterations, num_iterations))\n",
    "vae = VAE(784, vae_hidden_dim, latent_dim) # VAE is not reset after each iteration\n",
    "\n",
    "for iteration in range(num_iterations):\n",
    "    print(f\"Iteration {iteration + 1}/{num_iterations}\")\n",
    "\n",
    "    # Create DataLoader\n",
    "    dataloader = DataLoader(current_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Train the VAE\n",
    "    vae_optimizer = optim.Adam(vae.parameters(), lr=learning_rate)\n",
    "    for epoch in range(vae_num_epochs):\n",
    "        loss = train_vae(vae, vae_optimizer, dataloader)\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            print(f'VAE Epoch {epoch+1}/{vae_num_epochs}, Loss: {loss:.4f}')\n",
    "\n",
    "    # Initialize and train classifier. Note that the classifier is reinitialised every training loop\n",
    "    classifier = Classifier(784, classifier_hidden_dim, 10)\n",
    "    classifier_optimizer = optim.Adam(classifier.parameters(), lr=learning_rate)\n",
    "    for epoch in range(classifier_num_epochs):\n",
    "        loss, accuracy = train_classifier(classifier, classifier_optimizer, dataloader)\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            print(f'Classifier Epoch {epoch+1}/{classifier_num_epochs}, Loss: {loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    # Store the trained classifier in classifiers list. Used later for overall evaluation\n",
    "    classifiers.append(classifier)\n",
    "\n",
    "    # Generate new dataset\n",
    "    generated_digits = generate_digits(vae, num_samples)\n",
    "    generated_labels = classify_digits(classifier, generated_digits) # from prev classifier notably\n",
    "\n",
    "    # Visualize some generated digits. num_images represents the number of images to be displayed\n",
    "    visualize_digits(generated_digits[:num_images], generated_labels[:num_images], iteration + 1)\n",
    "\n",
    "    # Create new dataset for next iteration\n",
    "    current_dataset = TensorDataset(generated_digits, generated_labels) # torch.utils.data.TensorDataset alias\n",
    "\n",
    "    # Evaluate all previous classifiers on the new dataset, and writing to the loss and accuracy arrays\n",
    "    new_dataloader = DataLoader(current_dataset, batch_size=batch_size, shuffle=False) # shuffle should be unnecessary as the sampling from the latent space is random\n",
    "    for prev_iteration, prev_classifier in enumerate(classifiers):\n",
    "        loss, accuracy = evaluate_classifier(prev_classifier, new_dataloader)\n",
    "        loss_matrix[prev_iteration, iteration] = loss\n",
    "        accuracy_matrix[prev_iteration, iteration] = accuracy\n",
    "\n",
    "    # This line can be used to save the current dataset for later analysis\n",
    "    #torch.save(current_dataset, f'generated_dataset_iteration_{iteration + 1}.pt')\n",
    "\n",
    "# For smaller runs, use plot_matrix to display all relevant values\n",
    "colour_plot_matrix(loss_matrix, 'Loss Matrix')\n",
    "colour_plot_matrix(accuracy_matrix, 'Accuracy Matrix')\n",
    "\n",
    "# Save matrices for further analysis\n",
    "#np.save('loss_matrix.npy', loss_matrix)\n",
    "#np.save('accuracy_matrix.npy', accuracy_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
